---
title: "# Supplementary experiment - Data analysis: Reliability of the Serial Reaction Time task: If at first you don't succeed, try try try again"
author: "CÃ¡tia Margarida Oliveira, Marianna E. Hayiou-Thomas, Lisa Henderson; University of York"
output:
  html_document:
    code_folding: hide 
    number_sections: false
    theme: paper
    toc: true
    toc_float: true
---
  
```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE)
devtools::source_gist("c83e078bf8c81b035e32c3fc0cf04ee8", 
                      filename = 'render_toc.R')
```

### Import functions

```{r}

library(here)
source(here("utils.R"))
```

### Install retimes

```{r install retimes, echo = FALSE, warning = FALSE}
# Add retimes - it has been removed from CRAN

retimes <- "https://cran.r-project.org/src/contrib/Archive/retimes/retimes_0.1-2.tar.gz"
install.packages(retimes, repos=NULL, type="source")

```

### Importing libraries

```{r echo = T,warning=FALSE,message=FALSE}

listOfPackages <- c("BlandAltmanLeh", "tidyverse", "dplyr", "magrittr", "lme4", "irr", "ggExtra", "psych", "ggplot2", "data.table", "stringr", "lmerTest", "dfoptim", "optimx", "stats", "GLMMadaptive", "EnvStats", "sjPlot", "emmeans", "patchwork", "sjPlot", "sjmisc", "tidyr", "broom", "LaplacesDemon", "broom.mixed", "DHARMa", "retimes", "BayesFactor", "data.table")

ipak(listOfPackages)
```

# SRT

```{r echo = F, results = 'hide',warning=FALSE,message=FALSE}

Data <- read.csv("../data/Data_SuppExp.csv", header = TRUE)

#add missing columns-------

#assign 0 or 1 to odd or even trials
Data$Even_Odd <- rep(c("0", "1"), length.out=nrow(Data)) #even-odd column for split half

Data$RT <- Data$RT*1000 #multiply RT by 1000 for RT in milliseconds
Data$logRT <- log(Data$RT) #add log RT for modelling
Data$Group <- substr(as.character(Data$task), #add group column 
                     start= 1, 
                     stop= nchar(as.character(Data$task))-2)

# Descriptive
# Number of participants per group and session
Data %>%
  group_by(Session, Group) %>%
  summarise(count = n_distinct(Participant))

# Age, sample size and group of the participants
desc <- Data %>%
  group_by(Participant) %>%
  summarise(age = mean(Age), N = n(), Group = Group[1])

Group <- desc %>%
  dplyr::select(Participant, Group)

#Data <- subset(Data, Exp_Q1 == "n")
#Data <- subset(Data, Exp_Q1 == "y" & Exp_Q2 == 1)

#select for last 600 trials

Data <- subset(Data, Epoch > 2)

#To estimate interval between sessions
Data <- tidyr::separate(Data, col = date, into = c("day","time"), sep = "_")

Interval <- Data %>%
  group_by(Participant, Session) %>%
  summarise(date = day[2])

Interval_wide <- reshape2::dcast(Interval, Participant ~ Session, 
                                 value.var = c("date"))

names(Interval_wide)[names(Interval_wide) == "1"] <- "date1"
names(Interval_wide)[names(Interval_wide) == "2"] <- "date2"

Interval_wide$interval<- as.numeric(as.Date(Interval_wide$date2) - as.Date(Interval_wide$date1))
Interval_wide$date1 <- NULL
Interval_wide$date2 <- NULL
Data <- as.data.frame(list(Data, Interval_wide) %>%
                        reduce(full_join, by = "Participant"))

#Data <- subset(Data, interval > 7)
#Data$AM_PM <- if_else(as.numeric(substr(Data$time,1,2))>5 & as.numeric(substr(Data$time,1,2)) < 16, "AM", "PM")

#divide data in separate groups for outlier detection -------------

ISI <- subset(Data, Group == "ISI")

Session1_Data <- subset(Data, Session == 1)
Session2_Data <- subset(Data, Session == 2)

nrow(Session1_Data)
nrow(Session2_Data)

Session1_Prob <- subset(Session1_Data, Probability == 2)
Session1_Improb <- subset(Session1_Data, Probability == 1)
Session2_Prob <- subset(Session2_Data, Probability == 2)
Session2_Improb <- subset(Session2_Data, Probability == 1)

#Outliers by group------------------------------------

# PARTICIPANT OUTLIER REMOVAL BASED ON ACCURACY

#ISI group-------------

ISI_1 <- subset(Session1_Data, Group == "ISI")
ISI_2 <- subset(Session2_Data, Group == "ISI")

#Accuracy
pptmeanacc_ISI <-aggregate(Accuracy ~ Participant, ISI_1, sum)
pptmeanacc2_ISI <-aggregate(Accuracy ~ Participant, ISI_2, sum)

#Z scores

pptmeanacc_ISI$AccZ <-scale(pptmeanacc_ISI$Accuracy, center = TRUE, scale = TRUE) #One outlier - BC48907
pptmeanacc2_ISI$AccZ2 <-scale(pptmeanacc2_ISI$Accuracy, center = TRUE, scale = TRUE) #One outlier - BC48907

#noISI Group -------------------

noISI_1 <- subset(Session1_Data, Group == "noISI")
noISI_2 <- subset(Session2_Data, Group == "noISI")

#Accuracy
pptmeanacc_noISI <-aggregate(Accuracy ~ Participant, noISI_1, sum)
pptmeanacc2_noISI <-aggregate(Accuracy ~ Participant, noISI_2, sum)

pptmeanacc_noISI$AccZ <- scale(pptmeanacc_noISI$Accuracy, center = TRUE, scale = TRUE) #two outliers - LWM92483, SF5990
pptmeanacc2_noISI$AccZ2 <- scale(pptmeanacc2_noISI$Accuracy, center = TRUE, scale = TRUE) #two outliers - LWM92483	and DO12606

#Removal outlier participants based on accuracy
#session1
ISI_1 <- subset(ISI_1, Participant != "BC48907")
noISI_1 <- subset(noISI_1, Participant != "LWM92483")
noISI_1 <- subset(noISI_1, Participant != "SF5990")

#session2
ISI_2 <- subset(ISI_2, Participant != "BC48907")
noISI_2 <- subset(noISI_2, Participant != "LWM92483")
noISI_2 <- subset(noISI_2, Participant != "DO12606")

# Participant outlier removal based on Response Times------------------------
#---- ISI

# Mean response times 
pptmean_RT1_ISI <-aggregate(RT ~ Participant, data= ISI_1, mean)
pptmean_RT2_ISI <-aggregate(RT ~ Participant, data= ISI_2, mean)

####z scores
pptmean_RT1_ISI$RTZ <-scale(pptmean_RT1_ISI$RT, center = TRUE, scale = TRUE) #one outlier - MC86247
pptmean_RT2_ISI$RTZ <-scale(pptmean_RT2_ISI$RT, center = TRUE, scale = TRUE) #one outlier - MC86247

#---- no ISI

# Mean response times 
pptmean_RT1_noISI <-aggregate(RT ~ Participant, data= noISI_1, mean)
pptmean_RT2_noISI <-aggregate(RT ~ Participant, data= noISI_2, mean)

####z scores
pptmean_RT1_noISI$RTZ <-scale(pptmean_RT1_noISI$RT, center = TRUE, scale = TRUE) #one outlier - EM77607 and MM57769
pptmean_RT2_noISI$RTZ <-scale(pptmean_RT2_noISI$RT, center = TRUE, scale = TRUE) #one outlier -EM77607

#Removal outlier participants
#session1
ISI_1 <- subset(ISI_1, Participant != "MC86247")
noISI_1 <- subset(noISI_1, Participant != "EM77607")

#session2
ISI_2 <- subset(ISI_2, Participant != "MC86247")
noISI_2 <- subset(noISI_2, Participant != "EM77607")

Data.trimmed <- rbind(ISI_1, ISI_2, noISI_1, noISI_2)

Data.trimmed2 <- subset(Data.trimmed, Epoch > 2)
```

# Preprocessing

```{r echo = F, results = 'hide',warning=FALSE,message=FALSE}

# Descriptive statistics ------------------------------------------------------------

Data.trimmed$Probability[Data.trimmed$Probability == 2] <- "Prob"
Data.trimmed$Probability[Data.trimmed$Probability == 1] <- "Improb"

Age <- Data.trimmed %>%
  group_by(Group, Session) %>%
  summarise(mean = mean(na.omit(Age)), sd = sd(na.omit(Age)))

Desc <- Data.trimmed %>%
  group_by(Participant, Session, Probability, Group) %>%
  summarise(mean = mean(RT), Age = mean(Age), interval = mean(interval))

Wide <- Desc %>%
  group_by(Participant) %>%
  unite(Type, Probability, Session, sep = "_") %>%
  spread(Type, mean)

Wide <- mutate(Wide, Difference1 = Improb_1 - Prob_1,
               Difference2 = Improb_2 - Prob_2,
               Ratio1.1 = (Improb_1 - Prob_1)/((Improb_1 + Prob_1)/2),
               Ratio2.1 = (Improb_2 - Prob_2)/((Improb_2 + Prob_2)/2),
               Ratio1.2 = (Improb_1 - Prob_1)/Improb_1,
               Ratio2.2 = (Improb_2 - Prob_2)/Improb_2)

#for split-half reliability
Desc2 <- Data.trimmed %>%
  group_by(Participant, Session, Probability, Group, Even_Odd) %>%
  summarise(mean = mean(RT), Age = mean(Age))

Wide2 <- Desc2 %>%
  group_by(Participant) %>%
  unite(Type, Probability, Session, sep = "_") %>%
  spread(Type, mean)

Wide2 <- mutate(Wide2, Difference1 = Improb_1 - Prob_1,
                Difference2 = Improb_2 - Prob_2,
                Ratio1.1 = (Improb_1 - Prob_1)/((Improb_1 + Prob_1)/2),
                Ratio2.1 = (Improb_2 - Prob_2)/((Improb_2 + Prob_2)/2),
                Ratio1.2 = (Improb_1 - Prob_1)/Improb_1,
                Ratio2.2 = (Improb_2 - Prob_2)/Improb_2)

```

# Regression slopes

```{r computing regression slopes, echo = F, results = 'hide',warning=FALSE,message=FALSE}
Data.trimmed$Probability[Data.trimmed$Probability == "Prob"] <- "0"
Data.trimmed$Probability[Data.trimmed$Probability == "Improb"] <- "1"

# Set contrast coding
Data.trimmed$Group <- factor(Data.trimmed$Group)
contrasts(Data.trimmed$Group) <-  c(1,0) # ISI group

# ISI regression slopes ----------------------------------------------

Session1_Data <- subset(Data.trimmed, Session == 1)
Session2_Data <- subset(Data.trimmed, Session == 2)

#session 1
model1.ISI <- glmer(logRT ~ Probability*Group + (Probability|Participant),data= Session1_Data, family = Gamma, control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

df.ISI <- as.data.frame(coef(model1.ISI)$Participant) %>%
  tibble::rownames_to_column("Participant") %>% dplyr::rename(Diff1_slopes = Probability1) %>%
  dplyr::select(Participant, Diff1_slopes)

df.ISI <- merge(df.ISI, Group, by = c("Participant")) %>%
  subset(Group == "ISI") %>%
  dplyr::select(Participant, Diff1_slopes)

# session 2
model2.ISI <- glmer(logRT ~ Probability*Group + (Probability|Participant),data= Session2_Data, family = Gamma, control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

df2.ISI <- as.data.frame(coef(model2.ISI)$Participant) %>%
  tibble::rownames_to_column("Participant") %>%
  dplyr::rename(Diff2_slopes = Probability1) %>%
  dplyr::select(Participant, Diff2_slopes)

df2.ISI <- merge(df2.ISI, Group, by = c("Participant")) %>%
  subset(Group == "ISI") %>%
  dplyr::select(Participant, Diff2_slopes)

# Set contrasts
Data.trimmed$Group <- factor(Data.trimmed$Group)
contrasts(Data.trimmed$Group) <-  c(0,1) # ISI group

# ISI regression slopes ----------------------------------------------

Session1_Data <- subset(Data.trimmed, Session == 1)
Session2_Data <- subset(Data.trimmed, Session == 2)

# noISI regression slopes ----------------------------------------------

#session 1
model1.noISI <- glmer(logRT ~ Probability*Group + (Probability|Participant),data = Session1_Data, family = Gamma, control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

df.noISI <- as.data.frame(coef(model1.noISI)$Participant) %>%
  tibble::rownames_to_column("Participant") %>%
  rename(Diff1_slopes = Probability1) %>%
  select(Participant, Diff1_slopes)

df.noISI <- merge(df.noISI, Group, by = c("Participant")) %>%
  subset(Group == "noISI") %>%
  dplyr::select(Participant, Diff1_slopes)

# session 2
model2.noISI <- glmer(logRT ~ Probability*Group + (Probability|Participant),data = Session2_Data, family = Gamma, control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

df2.noISI <- as.data.frame(coef(model2.noISI)$Participant) %>%
  tibble::rownames_to_column("Participant") %>%
  rename(Diff2_slopes = Probability1) %>%
  select(Participant, Diff2_slopes)

df2.noISI <- merge(df2.noISI, Group, by = c("Participant")) %>%
  subset(Group == "noISI") %>%
  dplyr::select(Participant, Diff2_slopes)

Slopes1 <- rbind(df.ISI, df.noISI)
Slopes2 <- rbind(df2.ISI, df2.noISI)

```

# Outlier removal by group

```{r, echo = F, results = 'hide',warning=FALSE,message=FALSE}

#combine difference scores in dataframe
SL.data <- as.data.frame(list(Wide, Slopes1, Slopes2) %>%
                           reduce(full_join, by = "Participant"))

# Sequence learning outliers --------------------------------

# ISI

Diff_ISI <- subset(SL.data, Group == "ISI")


Diff_ISI_trim <- data.frame(Diff_ISI, stringsAsFactors = FALSE)

#identify outlier datapoints
cols <- paste0(c("Difference1", "Difference2", "Ratio1.1", "Ratio2.1", "Ratio1.2", "Ratio2.2", "Diff1_slopes", "Diff2_slopes"))
out_ISI <- sapply(Diff_ISI_trim[cols], remove_outliers)
Diff_ISI_trim[cols][out_ISI] <- NA

Diff_ISI_trimmed <- type.convert(Diff_ISI_trim)

# no ISI

Diff_noISI <- subset(SL.data, Group == "noISI")

Diff_noISI_trim <- data.frame(Diff_noISI, stringsAsFactors = FALSE)

#identify outlier datapoints
out_noISI <- sapply(Diff_noISI_trim[cols], remove_outliers)
Diff_noISI_trim[cols][out_noISI] <- NA

Diff_noISI_trimmed <- type.convert(Diff_noISI_trim)

# Merge dataframes by group
Difference <- rbind(Diff_ISI_trimmed, Diff_noISI_trimmed)

```

# Attention

```{r, echo = F, results = 'hide',warning=FALSE,message=FALSE}

# Attention ----------------------------------------------------------------

Data_PVT <- read.csv("../data/PVT_results_SuppExp.csv", header = TRUE)
Data_PVT <- subset(Data_PVT, RT != 0)
Data_PVT$RT <- Data_PVT$RT*1000
Data_PVT$Reciprocal <- 1/(Data_PVT$RT/1000)

#assign 0 or 1 to odd or even trials
Data_PVT$Even_Odd <- rep(c("0", "1"), length.out=nrow(Data_PVT))

#Attention measures long format

PVT_desc  <- Data_PVT %>% 
  group_by(Participant, Session) %>%
  summarise(false_start = sum(str_count(key_resp.keys, '\\bspace\\b')), 
            all_false_start = sum(str_count(key_resp.keys, '\\b\\w+\\b')),
            lapses = sum(RT >= 500),
            mean_PVT = mean(RT),
            median = median(RT),
            mean_recip = mean(Reciprocal))

#Attention measures wide format

setDT(PVT_desc)   # coerce to data.table
PVT_wide <- dcast(PVT_desc, Participant ~ Session, 
                  value.var = c("lapses", "mean_PVT", "mean_recip", "median"))
#standardise
PVT_wide_Scale <- PVT_wide  %>%
  mutate_at(c(2,3,4,5,6,7,8,9), funs(c(scale(.))))

#eliminate outlier datapoints

df <- data.frame(PVT_wide, stringsAsFactors = FALSE)

cols_pvt <- paste0(c("lapses_1", "lapses_2", "mean_PVT_1", "mean_PVT_2", "mean_recip_1", "mean_recip_2", "median_1", "median_2"))
mat <- sapply(df[cols_pvt], remove_outliers)
df[cols_pvt][mat] <- NA

PVT_trimmed <- type.convert(df)

#Split-half reliability

#Attention measures long format

PVT_desc_SH  <- Data_PVT %>% 
  group_by(Participant, Session, Even_Odd) %>%
  summarise(false_start = sum(str_count(key_resp.keys, '\\bspace\\b')), 
            all_false_start = sum(str_count(key_resp.keys, '\\b\\w+\\b')),
            lapses = sum(RT >500),
            mean_PVT = mean(RT),
            median = median(RT),
            mean_recip = mean(Reciprocal))

PVT_desc_SH$Even_Odd[PVT_desc_SH$Even_Odd == 0] <- "Even"
PVT_desc_SH$Even_Odd[PVT_desc_SH$Even_Odd == 1] <- "Odd"

#Attention measures wide format

setDT(PVT_desc_SH)   # coerce to data.table
PVT_wide_SH <- dcast(PVT_desc_SH, Participant ~ Session + Even_Odd, 
                     value.var = c("lapses", "mean_PVT", "mean_recip", "median"))
#standardise
PVT_wide_Scale_SH <- PVT_wide_SH  %>%
  mutate_at(c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17), funs(c(scale(.))))

#eliminate outlier datapoints

df_SH <- data.frame(PVT_wide_SH, stringsAsFactors = FALSE)

cols_pvt_SH <- paste0(c("lapses_1_Even", "lapses_1_Odd", "lapses_2_Even", "lapses_2_Odd", "mean_PVT_1_Even", "mean_PVT_1_Odd", "mean_PVT_2_Even", "mean_PVT_2_Odd", "mean_recip_1_Even", "mean_recip_1_Odd", "mean_recip_2_Even", "mean_recip_2_Odd", "median_1_Even", "median_1_Odd", "median_2_Even", "median_2_Odd"))
out_SH <- sapply(df_SH[cols_pvt_SH], remove_outliers)
df_SH[cols_pvt_SH][out_SH] <- NA

PVT_trimmed_SH <- type.convert(df_SH)

# #Ex-Gaussian with PVT task--------------------

Ex_gaussian_PVT <- as.data.frame(Data_PVT %>%
                                   group_by(Participant, Session) %>%
                                   summarise(Ex_gaussian_PVT_mu = retimes::mexgauss(RT)[1],
                                             Ex_gaussian_PVT_sigma = mexgauss(RT)[2],
                                             Ex_gaussian_PVT_tau = retimes::mexgauss(RT)[3]))

Ex_gaussian_PVT$scale_tau <- scale(Ex_gaussian_PVT$Ex_gaussian_PVT_tau)
Ex_gaussian_PVT_trimmed <- subset(Ex_gaussian_PVT, scale_tau < 2.5)
Ex_gaussian_PVT_trimmed$scale_tau <- NULL

# #Attention measures wide format
# 
setDT(Ex_gaussian_PVT_trimmed)   # coerce to data.table
PVT_tau_trimmed <- dcast(Ex_gaussian_PVT_trimmed, Participant ~ Session, 
                         value.var = c("Ex_gaussian_PVT_mu", "Ex_gaussian_PVT_sigma", "Ex_gaussian_PVT_tau"))
```

# Explicit awareness data

```{r, echo = F, results = 'hide',warning=FALSE,message=FALSE}

Explicit_data <- read.csv("../data/Explicit_awareness_data_SuppExp.csv", header = TRUE)

Explicit_data_ISI <- subset(Explicit_data, Group == "ISI")

#standardise
Explicit_data_ISI_scale  <- Explicit_data_ISI  %>%
  mutate_at(c(3,4,5,6,7,8), funs(c(scale(.))))

#eliminate outlier datapoints

Exp_data_ISI <- data.frame(Explicit_data_ISI, stringsAsFactors = FALSE)

cols_exp_ISI <- paste0(c("Triplets_exc", "Diff_triplets_exc", "Longest_exc", "Triplets_inc", "Diff_triplets_inc", "Longest_inc"))
mat_ISI <- sapply(Exp_data_ISI[cols_exp_ISI], remove_outliers)
Exp_data_ISI[cols_exp_ISI][mat_ISI] <- NA

Explicit_trimmed_ISI <- type.convert(Exp_data_ISI)

#no ISI
Explicit_data_noISI <- subset(Explicit_data, Group == "noISI")

#standardise
Explicit_data_noISI_scale  <- Explicit_data_noISI  %>%
  mutate_at(c(3,4,5,6,7,8), funs(c(scale(.))))

#eliminate outlier datapoints

Exp_data_noISI <- data.frame(Explicit_data_noISI, stringsAsFactors = FALSE)

cols_exp_noISI <- paste0(c("Triplets_exc", "Diff_triplets_exc", "Longest_exc", "Triplets_inc", "Diff_triplets_inc", "Longest_inc"))
mat_noISI <- sapply(Exp_data_noISI[cols_exp_noISI], remove_outliers)
Exp_data_noISI[cols_exp_noISI][mat_noISI] <- NA

Explicit_trimmed_noISI <- type.convert(Exp_data_noISI)

#joining explicit awareness dataframes

Explicit_trimmed <- rbind(Explicit_trimmed_ISI, Explicit_trimmed_noISI)

Explicit_trimmed$Group <- NULL

#Join difference dataframe and attention dataframes --------------------

Cor_data <- as.data.frame(list(Difference, PVT_trimmed, PVT_tau_trimmed, Explicit_trimmed) %>%
                            reduce(full_join, by = c("Participant")))

#Add group information to participants with missing information

Group <- Data %>% group_by(Participant) %>%
                summarise(Group = Group[1])

Age <- Data %>% group_by(Participant) %>%
                summarise(Age = mean(Age))

Cor_data <- as.data.frame(list(Cor_data, Group, Age) %>%
                             reduce(full_join, by = "Participant"))

Cor_data <- Cor_data %>%
  mutate(Group.x= coalesce(Group.x,Group.y), Age.x= coalesce(Age.x,Age.y))

#Remove unnecessary columns
Cor_data$Group.y <- NULL
Cor_data$Age.y <- NULL

Cor_data$X <- NULL
Cor_data$Sequence <- NULL

names(Cor_data)[names(Cor_data) == "Group.x"] <- "Group"
names(Cor_data)[names(Cor_data) == "Age.x"] <- "Age"

describe.by(Cor_data$Age, Cor_data$Group)
```

# Response times per group

``` {r echo = F, warning=FALSE,message=FALSE}

RT <- Data.trimmed %>%
  group_by(Probability, Epoch, Session, Group) %>%
  summarise(mean = mean(RT, na.rm = TRUE),
            sd = sd(RT, na.rm = TRUE),
            n = length(which(!is.na(RT)))) %>%
  mutate(se = sd / sqrt(n),
         lower.ci= mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se)


dcast(setDT(RT), Epoch~Probability+Group+Session, value.var=c('mean'))

N <- Data.trimmed %>% group_by(Participant, Probability, Session) %>% summarise(N = n())

```

# Plotting Response times

``` {r echo = F, warning=FALSE,message=FALSE}

RT <- Data.trimmed %>%
  group_by(Probability, Epoch, Session, Group) %>%
  summarise(mean = mean(RT, na.rm = TRUE),
            sd = sd(RT, na.rm = TRUE),
            n = length(which(!is.na(RT)))) %>%
  mutate(se = sd / sqrt(n),
         lower.ci= mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper.ci = mean + qt(1 - (0.05 / 2), n - 1) * se)

RT <- RT %>% 
  mutate(series = interaction(Probability, Group, sep = "_"))

RT$Probability <- as.numeric(RT$Probability)
RT$Probability[RT$Probability == 0] <- "Prob"
RT$Probability[RT$Probability == 1] <- "Improb"

RT$Epoch <- as.factor(RT$Epoch)

RT.plot <- ggplot(RT, aes(x = Epoch, y = mean, group = series, color = Group)) +
  geom_line(aes(linetype = Probability), position=position_dodge(0.05), size = 1) +
  geom_errorbar(aes(ymin=lower.ci, ymax=upper.ci), width=.05, position=position_dodge(0.05), show.legend = FALSE) +
  facet_wrap(~ Session, scales="free") +
  coord_cartesian(ylim = c(350, 520)) +
  theme_classic()

RT.plot <- RT.plot + ylab("mean RT") + xlab("Epoch")

# Save plot
RT.plot

``` 


## ISI group 

### Correlations

```{r echo = F, results = 'markup'}

Cor_data_ISI <- subset(Cor_data, Group == "ISI")

Cor_data_ISI$Participant <- NULL
Cor_data_ISI$Group <- NULL
Cor_data_ISI$Even_Odd <- NULL
res_ISI <- as.data.frame(cor(Cor_data_ISI, method = "pearson", use = "pairwise.complete.obs"))
res_p_ISI <- Hmisc::rcorr(as.matrix(Cor_data_ISI))
#write.csv(res_p_ISI$P, "ISIcorr_pvalues.csv")

lower_ISI<-res_ISI
lower_ISI[lower.tri(res_ISI, diag=TRUE)]<-""
lower_ISI<-as.data.frame(lower_ISI)
print(lower_ISI)

#write.csv(lower_ISI, "../results/ISI_correlations.csv")
```

## Agreement - ISI group

``` {r echo = F}
#Bland-Atlman with adjustable axis

#session 1 and 2
ba.stats_ISI <- bland.altman.stats(Diff_ISI_trimmed$Difference1, Diff_ISI_trimmed$Difference2)
ba.stats_ISI$lines
ba.stats_ISI$CI.lines

ba.stats2_ISI <- as.data.frame(cbind(ba.stats_ISI$means, ba.stats_ISI$diffs))

#check if means and difference between means are correlated
cor.test(ba.stats_ISI$means, ba.stats_ISI$diffs)

#draw plot 
BA_ISI <- ggMarginal(ggplot(ba.stats2_ISI, aes(V1,V2)) +
                       geom_hline(yintercept = ba.stats_ISI$lines, linetype="dashed",   size=1, col = "black")+
                       geom_point(colour = "dodgerblue3", size = 2.5) +
                       theme_bw()+
                       theme(legend.position = "bottom", legend.box = "vertical") +
                       labs(color = "Levenshtein distance  ") +
                       scale_y_continuous(breaks = seq(-70,100,20)) +
                       coord_cartesian(ylim = c(-68,100), xlim = c(0, 70)) +
                       labs(x = "\n Average procedural learning", y = "Difference in procedural learning between sessions (S2-S1)") +
                       geom_hline(yintercept = ba.stats_ISI$CI.lines, col = "grey", linetype="dashed", size = 0.70), type = "histogram")

print(BA_ISI)
```

``` {r echo = F, results = 'hide'}

# Bias 
Diff_ISI_trimmed$Stability1 <- Diff_ISI_trimmed$Diff2_slopes - Diff_ISI_trimmed$Diff1_slopes

#ploting stability
hist(Diff_ISI_trimmed$Stability1)

shapiro.test(Diff_ISI_trimmed$Stability1) # normally distributed

#Comparing stability to zero
#mean
t.test(Diff_ISI_trimmed$Stability1, mu=0)

#Bland-Altman's 95% limits of agreement

#Session 1 and 2
LOA1_lowerbound_ISI <-  mean(na.omit(Diff_ISI_trimmed$Stability1)) - (1.96 * sd(na.omit(Diff_ISI_trimmed$Stability1)))
LOA1_upperbound_ISI <-  mean(na.omit(Diff_ISI_trimmed$Stability1)) + (1.96 * sd(na.omit(Diff_ISI_trimmed$Stability1)))

```


### Bayesian correlations - ISI group

```{r ISI Bayesian correlations, eval = FALSE, warning=FALSE, message=FALSE}

# Select variables for Bayesian correlations
BFcols <- c("Diff1_slopes", "Diff2_slopes", "median_1", "median_2", "Ex_gaussian_PVT_tau_1", "Ex_gaussian_PVT_tau_2", "Age", "Triplets_inc","Triplets_exc")

#ISI.BF.df <- Cor_data_ISI[BFcols]

# function to get correlation between two variables
ISI.BFcor = function(x,y) extractBF(correlationBF(ISI.BF.df[,x], ISI.BF.df[,y]))$bf
ISI.BFcor = Vectorize(ISI.BFcor)

# function to get lower credible interval between two variables
ISI.lb.CIcor = function(x,y) summary(correlationBF(x = ISI.BF.df[,x], y = ISI.BF.df[,y],
                                               posterior = TRUE, iterations = 100000))$quantiles[[1]]
ISI.lb.CIcor = Vectorize(ISI.lb.CIcor)

# function to get upper credible interval between two variables
ISI.ub.CIcor = function(x,y) summary(correlationBF(x = ISI.BF.df[,x], y = ISI.BF.df[,y],
                                               posterior = TRUE, iterations = 100000))$quantiles[[9]]
ISI.ub.CIcor = Vectorize(ISI.ub.CIcor)

# Compute correlations for all pairwise variables of interest
ISI.Bayes.factors <- expand.grid(V1 =names(ISI.BF.df), V2=names(ISI.BF.df))%>%
  filter(V1 != V2) %>%   # keep pairs of names where v1 matches v2, but are not the same
  mutate(BF = ISI.BFcor(V1, V2), lb.CI = ISI.lb.CIcor(V1,V2), ub.CI = ISI.ub.CIcor(V1,V2)) %>%   # for those pairs (only) obtain correlation value
  subset(V1 == "Diff1_slopes"|V1 == "Diff2_slopes"| V1 == "Diff3_slopes")


samples <- summary(correlationBF(x = ISI.BF.df$Diff1_slopes, y = ISI.BF.df$Diff2_slopes,
                        posterior = TRUE, iterations = 100000))

write.csv(ISI.Bayes.factors, "ISI.Bayes.factors.csv")
```

## noISI group 

### Correlations

``` {r echo = F, results = 'hide'}

Cor_data_noISI <- subset(Cor_data, Group == "noISI")

Cor_data_noISI$Participant <- NULL
Cor_data_noISI$Group <- NULL
Cor_data_noISI$Even_Odd <- NULL

res_noISI <- round(as.data.frame(cor(Cor_data_noISI, method = "pearson", use = "pairwise.complete.obs")), 3)

res_p_noISI <- Hmisc::rcorr(as.matrix(Cor_data_noISI))

#write.csv(res_p_noISI$P, "../results/noISIcorr_pvalues.csv")
```

```{r, echo = F}
lower_noISI<-res_noISI
lower_noISI[lower.tri(res_noISI, diag=TRUE)]<-""
lower_noISI<-as.data.frame(lower_noISI)
print(lower_noISI)

write.csv(lower_noISI, "../results/noISI_correlations.csv")

```


## Agreement - noISI group 

```{r echo = F}

#Bland-Atlman with adjustable axis

#session 1 and 2
ba.stats_noISI <- bland.altman.stats(Diff_noISI_trimmed$Difference1, Diff_noISI_trimmed$Difference2)
ba.stats_noISI$lines
ba.stats_noISI$CI.lines

ba.stats2_noISI <- as.data.frame(cbind(ba.stats_noISI$means, ba.stats_noISI$diffs))

#check if means and difference between means are correlated
cor.test(ba.stats_noISI$means, ba.stats_noISI$diffs)

#draw plot
BA_noISI <- ggMarginal(ggplot(ba.stats2_noISI, aes(V1,V2)) +
                         geom_hline(yintercept = ba.stats_noISI$lines, linetype="dashed", size=1, col = "black") +
                         geom_point(colour = "dodgerblue3", size = 2.5) +
                         theme_bw() + theme(legend.position = "bottom", legend.box = "vertical") +
                         labs(color = "Levenshtein distance  ") +
                         scale_y_continuous(breaks = seq(-70,100,20)) +
                         coord_cartesian(ylim = c(-68,100), xlim = c(0,70)) +
                         labs(x = "\n Average procedural learning", y = "\nDifference in procedural learning between sessions (S2-S1)") +
                         geom_hline(yintercept = ba.stats_noISI$CI.lines, col = "grey", linetype="dashed", size = 0.70),
                         type = "histogram")

BA_noISI

grid <- cowplot::plot_grid(BA_ISI, BA_noISI, ncol = 2, labels = c("A", "B"))

grid

```

``` {r echo = F, results = 'hide'}

# Bias 
Diff_noISI_trimmed$Stability1 <- Diff_noISI_trimmed$Diff2_slopes - Diff_noISI_trimmed$Diff1_slopes

#ploting stability
#hist(Diff_noISI$Stability1)

shapiro.test(Diff_noISI_trimmed$Stability1) # normally distributed

#Comparing stability to zero
#mean
t.test(Diff_noISI_trimmed$Stability1, mu=0)

#Bland-Altman's 95% limits of agreement

#Session 1 and 2
LOA1_lowerbound_noISI <-  mean(na.omit(Diff_noISI_trimmed$Stability1)) - (1.96 * sd(na.omit(Diff_noISI_trimmed$Stability1)))
LOA1_upperbound_noISI <-  mean(na.omit(Diff_noISI_trimmed$Stability1)) + (1.96 * sd(na.omit(Diff_noISI_trimmed$Stability1)))

```


### Bayesian correlations

```{r noISI Bayesian correlations, eval = FALSE, warning=FALSE, message=FALSE}

#noISI.BF.df <- Cor_data_noISI[BFcols]

# function to get correlation between two variables
noISI.BFcor = function(x,y) extractBF(correlationBF(noISI.BF.df[,x], noISI.BF.df[,y]))$bf
noISI.BFcor = Vectorize(noISI.BFcor)

# function to get lower credible interval between two variables
noISI.lb.CIcor = function(x,y) summary(correlationBF(x = noISI.BF.df[,x], y = noISI.BF.df[,y],
                                               posterior = TRUE, iterations = 100000))$quantiles[[1]]
noISI.lb.CIcor = Vectorize(noISI.lb.CIcor)


# function to get upper credible interval between two variables
noISI.ub.CIcor = function(x,y) summary(correlationBF(x = noISI.BF.df[,x], y = noISI.BF.df[,y],
                                               posterior = TRUE, iterations = 100000))$quantiles[[9]]
noISI.ub.CIcor = Vectorize(noISI.ub.CIcor)

# Compute correlations for all pairwise variables of interest
noISI.Bayes.factors <- expand.grid(V1 =names(noISI.BF.df), V2=names(noISI.BF.df))%>%
  filter(V1 != V2) %>%   # keep pairs of names where v1 matches v2, but are not the same
  mutate(BF = noISI.BFcor(V1, V2), lb.CI = noISI.lb.CIcor(V1,V2), ub.CI = noISI.ub.CIcor(V1,V2)) %>%   # for those pairs (only) obtain correlation value
  subset(V1 == "Diff1_slopes"|V1 == "Diff2_slopes"| V1 == "Diff3_slopes")


samples <- summary(correlationBF(x = noISI.BF.df$Diff1_slopes, y = noISI.BF.df$Diff2_slopes,
                        posterior = TRUE, iterations = 100000))

write.csv(noISI.Bayes.factors, "noISI.Bayes.factors.csv")

```

## Reliability by group

### Descriptive procedural learning

``` {r echo = F}

#creates long dataframe for ISI group

Difference_long <- Difference %>%
  select("Participant", "Group", "Age", "Difference1", "Difference2", "Ratio1.1", "Ratio2.1", "Ratio1.2", "Ratio2.2",
         "Diff1_slopes", "Diff2_slopes")

Diff_long <- reshape2::melt(Difference_long, id.vars=c("Participant", "Group", "Age"))

#summarise data - means and SD

Desc_SeqL <- Diff_long %>%
  group_by(Group,variable) %>%
  summarise(n = length(which(!is.na(value))), mean = mean(na.omit(value)), sd = sd(na.omit(value)))

Desc_SeqL <- Desc_SeqL %>%
  mutate_if(is.numeric, round, digits=3)
print(Desc_SeqL)

```

### Summary test-retest reliability

``` {r echo = F}

D1 <- cor.test(Diff_noISI_trimmed$Difference1, Diff_noISI_trimmed$Difference2)
R1.1 <- cor.test(Diff_noISI_trimmed$Ratio1.1, Diff_noISI_trimmed$Ratio2.1)
R1.2 <- cor.test(Diff_noISI_trimmed$Ratio1.2, Diff_noISI_trimmed$Ratio2.2)
RS1 <- cor.test(Diff_noISI_trimmed$Diff1_slopes, Diff_noISI_trimmed$Diff2_slopes, use = "complete.obs")
D2 <- cor.test(Diff_ISI_trimmed$Difference1, Diff_ISI_trimmed$Difference2)
R2.1 <- cor.test(Diff_ISI_trimmed$Ratio1.1, Diff_ISI_trimmed$Ratio2.1)
R2.2 <- cor.test(Diff_ISI_trimmed$Ratio1.2, Diff_ISI_trimmed$Ratio2.2)
RS2 <- cor.test(Diff_ISI_trimmed$Diff1_slopes, Diff_ISI_trimmed$Diff2_slopes)

Test_retest <- data.frame(Test_retest_reliability = c("Difference", "Ratio1", "Ratio2", "Regression slopes"),
                          ISI_N = c(D2$parameter+2, R2.1$parameter+2, R2.2$parameter+2, RS2$parameter+2),
                          ISI = round(c(D2$estimate, R2.1$estimate, R2.2$estimate, RS2$estimate),2),
                          noISI_N = c(D1$parameter+2, R1.1$parameter+2, R1.2$parameter+2, RS1$parameter+2),
                          no_ISI = round(c(D1$estimate, R1.1$estimate, R1.2$estimate, RS1$estimate),2))
print(Test_retest)

```

### Effect of group on reliability

```{r}

Difference$scaled.Diff2_slopes <- scale(Difference$Diff2_slopes)
Difference$scaled.Diff1_slopes <- scale(Difference$Diff1_slopes)

reliability.model.lm <- lm(scaled.Diff2_slopes ~ scaled.Diff1_slopes*factor(Group), data = Difference, REML=FALSE)
reliability.model.rlm <- MASS::rlm(scaled.Diff2_slopes ~ scaled.Diff1_slopes*factor(Group), data = Difference, REML=FALSE)
anova(reliability.model.lm, reliability.model.rlm)

summary(reliability.model.lm)
confint(reliability.model.lm)

#sfsmisc::f.robftest(reliability.model.rlm, var = "scaled.Diff1_slopes")
#sfsmisc::f.robftest(reliability.model.rlm, var = "factor(Group)")
#sfsmisc::f.robftest(reliability.model.rlm, var = "scaled.Diff1_slopes:Group")

interactions::interact_plot(reliability.model.lm, pred = scaled.Diff1_slopes, modx = Group,
                            interval = TRUE,
                            int.type = "confidence",
                            int.width = .8,
                            colors = c("black", "blue"),
                            x.label = "Random slopes - session 1",
                            y.label = "Random slopes - session 2")
```
